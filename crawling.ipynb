{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "937cda32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d4883cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/Users/abdulrafay/Desktop/Research Project/Data/Repo_URL/repo_urls.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51e94d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, 'rb') as f:\n",
    "    my_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0dd10a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reponame_author(url):\n",
    "    try:\n",
    "        pattern = r\"^https://github\\.com/([^/]+)/([^/]+)\"\n",
    "\n",
    "        match = re.search(pattern, url)\n",
    "\n",
    "        if match:\n",
    "            return match.group(1), match.group(2)\n",
    "        else:\n",
    "            return 'No author', 'no repository name' \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cbed9a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_about(soup):\n",
    "    try:\n",
    "        links = soup.find_all('h2', class_='mb-3 h4')\n",
    "        \n",
    "        if not links:\n",
    "            print(\"No sections with the class 'mb-3 h4' were found.\")\n",
    "            return\n",
    "        \n",
    "        about_found = False\n",
    "        for link in links:\n",
    "            if 'About' in link.get_text():\n",
    "                p = link.find_next_sibling('p', class_='f4 my-3')\n",
    "\n",
    "                if p:\n",
    "                   return p.get_text(strip=True)\n",
    "                else:\n",
    "                    return ''\n",
    "                    \n",
    "                about_found = True\n",
    "                break \n",
    "\n",
    "        if not about_found:\n",
    "            print(\"About section not found.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error occurred:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a778d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics(soup):\n",
    "    all_topics = []\n",
    "    topic_section = soup.find_all('h3', {'class': 'sr-only'})\n",
    "\n",
    "    for topic in topic_section:\n",
    "        if topic.text == 'Topics':\n",
    "            topic_link = topic.find_next_sibling('div', class_='my-3')\n",
    "            topic_tag = topic_link.find_all('a', class_='topic-tag topic-tag-link')\n",
    "            \n",
    "            for tag in topic_tag:\n",
    "                all_topics.append(tag.get_text(strip=True))\n",
    "\n",
    "    return all_topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49a2b3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stars(url):\n",
    "    base_url = url + \"/\" + \"stargazers\"\n",
    "\n",
    "    all_stars = []\n",
    "    page = 1\n",
    "\n",
    "    while True:\n",
    "        url = f\"{base_url}?page={page}\"\n",
    "        while True: \n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                break\n",
    "                    \n",
    "            else:\n",
    "                print(\"Sleeping...\")\n",
    "                time.sleep(2)\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        no_star = soup.find('h2', string=\"Be the first to star this repository\")\n",
    "        tag = soup.find('div', class_='clearfix container-xl px-3 px-md-4 px-lg-5 mt-4')\n",
    "        end_star = tag.find('p').text.strip().replace(\"\\n\", \" \")\n",
    "\n",
    "        if no_star or 'That’s it. You’ve reached the end of' in end_star:\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            star_links = soup.find_all('li', class_='col-md-4 mb-3')\n",
    "            \n",
    "            for star_link in star_links:\n",
    "                starer = star_link.find('span', class_='Truncate-text').text.strip()\n",
    "                about = star_link.find('p', class_='mb-3').text.strip().split('\\n', 1)[0]\n",
    "                all_stars.append({'user_name': starer, 'user_about': about})\n",
    "\n",
    "        page+=1\n",
    "    \n",
    "    return all_stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a6e6741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_watchers(url):\n",
    "    base_url = url + \"/\" + \"watchers\"\n",
    "\n",
    "    all_watchers = []\n",
    "    page = 1\n",
    "\n",
    "    while True:\n",
    "        url = f\"{base_url}?page={page}\"\n",
    "\n",
    "        while True: \n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                break\n",
    "                    \n",
    "            else:\n",
    "                print(\"Sleeping...\")\n",
    "                time.sleep(2)\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        no_watcher = soup.find('h2', class_=\"blankslate-heading\")\n",
    "\n",
    "        if no_watcher:\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            watcher_links = soup.find_all('li', class_='col-md-4 mb-3')\n",
    "            \n",
    "            for watcher_link in watcher_links:\n",
    "                watcher = watcher_link.find('span', class_='Truncate-text').text.strip()\n",
    "                about = watcher_link.find('p', class_='mb-3').text.strip().split('\\n', 1)[0]\n",
    "                all_watchers.append({'user_name': watcher, 'user_about': about})\n",
    "\n",
    "        page+=1\n",
    "\n",
    "    return all_watchers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f5a215a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_forks(url):\n",
    "    base_url = url + \"/\" + \"forks\"\n",
    "\n",
    "    all_forks = []\n",
    "    page = 1\n",
    "\n",
    "    while True:\n",
    "        url = f\"{base_url}?page={page}&period=&include=active%2Cinactive\"\n",
    "        \n",
    "        while True: \n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                break\n",
    "                    \n",
    "            else:\n",
    "                print(\"Sleeping...\")\n",
    "                time.sleep(2)\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        no_forks = soup.find('div', class_=\"Box mt-3\")\n",
    "\n",
    "        if not no_forks:\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            fork_links = no_forks.find_all('h2', class_=\"f4\")\n",
    "\n",
    "            for fork_link in fork_links:\n",
    "                author = fork_link.find('span',class_='f4 d-inline-block').get_text(strip=True)\n",
    "                forked_as = fork_link.find('a',class_='Link f4').get_text(strip=True)\n",
    "                active_inactive_tag = fork_link.find_next_sibling('div', class_='d-flex flex-column flex-sm-row mt-2')\n",
    "                active_flag = active_inactive_tag.find('span', class_='text-small')\n",
    "\n",
    "                if active_flag is not None and active_flag.get_text(strip=True) == \"Never updated\":\n",
    "                    active_flag = \"Inactive\"\n",
    "                else:\n",
    "                    active_flag = \"Active\"\n",
    "\n",
    "\n",
    "                all_forks.append({'user_name': author, 'repo_forked_as': forked_as, 'repo_status': active_flag})      \n",
    "\n",
    "        page+=1\n",
    "\n",
    "\n",
    "    return all_forks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9fd9e5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_langauges(soup):\n",
    "    links = soup.find_all('h2', class_='h4 mb-3')\n",
    "\n",
    "    languages = {}\n",
    "\n",
    "    for link in links:\n",
    "        if 'Languages' in link.text:\n",
    "            ul = link.find_next_sibling('ul', class_='list-style-none')\n",
    "            span1 = ul.find_all('span',class_='color-fg-default text-bold mr-1')\n",
    "\n",
    "            for i in span1:\n",
    "                key = i.text.strip()\n",
    "                value = i.find_next_sibling().text.strip()\n",
    "                languages[key] = value\n",
    "\n",
    "\n",
    "    return languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88fc0f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contributors(owner, repo, token):\n",
    "    url = f\"https://api.github.com/repos/{owner}/{repo}/contributors\"\n",
    "    headers = {'Authorization': f'token {token}'} if token else {}\n",
    "    page = 1\n",
    "    all_contributors = []\n",
    "\n",
    "    while True:\n",
    "        response = requests.get(url, headers=headers, params={'per_page': 100, 'page': page})\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            contributors = response.json()\n",
    "            if not contributors: \n",
    "                return all_contributors\n",
    "\n",
    "            for contributor in contributors:\n",
    "                all_contributors.append({'user_name': contributor['login'], 'no_of_commits': contributor['contributions']})\n",
    "\n",
    "            page += 1\n",
    "        else:\n",
    "            print(f\"Failed to fetch contributors: {response.status_code}\")\n",
    "            return None\n",
    "\n",
    "    return all_contributors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a25e8350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_issues(url):\n",
    "    base_url = url + \"/\" + \"issues\"\n",
    "    all_issues = []\n",
    "\n",
    "    states = ['open', 'closed']\n",
    "    \n",
    "    for state in states:\n",
    "        page = 1\n",
    "        while True:\n",
    "            url = f\"{base_url}?page={page}&q=is%3Aissue+is%3A{state}\"\n",
    "            while True: \n",
    "\n",
    "                response = requests.get(url)\n",
    "                if response.status_code == 200:\n",
    "                    break\n",
    "                \n",
    "                else:\n",
    "                    print(\"Sleeping...\")\n",
    "                    time.sleep(2)\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            no_issues_tag_open = soup.find('h2', string=\"There aren’t any open issues.\")\n",
    "            no_issues_tag_closed = soup.find('h2', string=\"No results matched your search.\")\n",
    "            no_issues_anyway = soup.find('h2', string='Welcome to issues!')\n",
    "\n",
    "            if no_issues_tag_open or no_issues_tag_closed or no_issues_anyway:\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                issue_links = soup.find_all('div', class_=\"flex-auto min-width-0 p-2 pr-3 pr-md-2\")\n",
    "\n",
    "                for issue in issue_links:\n",
    "                    title_tag = issue.find('a', class_='Link--primary')\n",
    "                    title = title_tag.text.strip()\n",
    "                    issue_id = title_tag['href'].split('/')[-1]\n",
    "                    author = issue.find('a', class_='Link--muted').text\n",
    "                    all_issues.append({'issue_id': issue_id, 'issue_title': title, 'issue_status': state, 'issue_author': author})\n",
    "\n",
    "            page+=1\n",
    "\n",
    "    return all_issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "750e7b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pull_requests(url):\n",
    "    base_url = url + \"/\" + \"pulls\"\n",
    "    all_pr = []\n",
    "\n",
    "    states = ['open', 'closed']\n",
    "    \n",
    "    for state in states:\n",
    "        page = 1\n",
    "        while True:\n",
    "            url = f\"{base_url}?page={page}&q=is%3Apr+is%3A{state}\"\n",
    "            while True: \n",
    "\n",
    "                response = requests.get(url)\n",
    "                if response.status_code == 200:\n",
    "                    break\n",
    "                \n",
    "                else:\n",
    "                    print(\"Sleeping...\")\n",
    "                    time.sleep(2)\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            no_PR_open = soup.find('h2', string=\"There aren’t any open pull requests.\")\n",
    "            no_PR_closed = soup.find('h2', string=\"No results matched your search.\")\n",
    "            no_pr_anyway = soup.find('h2', string='Welcome to pull requests!')\n",
    "\n",
    "            if no_PR_open or no_PR_closed or no_pr_anyway:\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                PR_links = soup.find_all('div', class_=\"flex-auto min-width-0 p-2 pr-3 pr-md-2\")\n",
    "\n",
    "                for pr in PR_links:\n",
    "                    title_tag = pr.find('a', class_='Link--primary')\n",
    "                    pr_title = title_tag.text.strip()\n",
    "                    pr_id = title_tag['href'].split('/')[-1]\n",
    "                    pr_author = pr.find('a', class_='Link--muted').text\n",
    "                    all_pr.append({'pr_id': pr_id, 'pr_title': pr_title, 'pr_status': state, 'pr_author': pr_author})\n",
    "\n",
    "            page+=1\n",
    "\n",
    "    return all_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe4564e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_json(url, author, repo_name, about, topic_lst, star_lst, watcher_lst, forks_lst, languages_lst, contributors_lst, issues_lst, pr_lst):\n",
    "    data = {\n",
    "        'url': url,\n",
    "        'author': author,\n",
    "        'repositoryName': repo_name,\n",
    "        'about': about,\n",
    "        'topics': topic_lst,\n",
    "        'languages': languages_lst,\n",
    "        'stars': star_lst,\n",
    "        'watchers': watcher_lst,\n",
    "        'forks': forks_lst,\n",
    "        'contributors': contributors_lst,\n",
    "        'issues': issues_lst,\n",
    "        'pull_requests': pr_lst\n",
    "    }\n",
    "\n",
    "    filename = './p_scaads_finetune/crawled_github_data/5000/' + repo_name + \".json\"\n",
    "\n",
    "    with open(filename, \"w\") as json_file:\n",
    "        json.dump(data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941de840",
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in my_list[:1]:\n",
    "    soup = BeautifulSoup(requests.get(url).content, 'html.parser')\n",
    "\n",
    "    author, repo_name = get_reponame_author(url)\n",
    "    about = get_about(soup)\n",
    "    topic_lst = get_topics(soup)\n",
    "    star_lst = get_stars(url)\n",
    "    watcher_lst = get_watchers(url)\n",
    "    forks_lst = get_forks(url)\n",
    "    languages_lst = get_langauges(soup)\n",
    "    contributors_lst = get_contributors(author, repo_name, '')\n",
    "    issues_lst = get_issues(url)\n",
    "    pr_lst = get_pull_requests(url)\n",
    "\n",
    "    make_json(url, author, repo_name, about, topic_lst, star_lst, watcher_lst, forks_lst, languages_lst, contributors_lst, issues_lst, pr_lst)\n",
    "    print(\"=====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c83ca02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
